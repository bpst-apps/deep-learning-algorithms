{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d757cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f2a7f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute weighted sum\n",
    "def wSum(X, W):\n",
    "    h = torch.from_numpy(X)\n",
    "    z = torch.matmul(W,h)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a4eedd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute weighted sum of neurons of a layer\n",
    "def forwardStep(X, W_list):\n",
    "    h = torch.from_numpy(X)\n",
    "    for W in W_list:\n",
    "        h = torch.matmul(W,h)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a08ec6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 10\n",
    "n = 1000\n",
    "X = np.random.rand(n, inputDim)\n",
    "y = np.random.randint(0, 2, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbaeca50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6d9b9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c935f51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e8de96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor(np.random.uniform(0, 1, inputDim), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33042387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2759, dtype=torch.float64, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = wSum(X[0,:], W)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469e7d29",
   "metadata": {},
   "source": [
    "#### Deep Neural Network Implementation without Activation Function\n",
    "\n",
    "**Intialize Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b79ed540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W1 is all the weigths vector for all the neurons in computation layer 1 \n",
    "W1 = torch.tensor(np.random.uniform(0, 1, (2, inputDim)), requires_grad=True)\n",
    "\n",
    "# W2 is all the weigths vector for all the neurons in computation layer 2 \n",
    "W2 = torch.tensor(np.random.uniform(0, 1, (3, 2)), requires_grad=True)\n",
    "\n",
    "# W3 is all the weigths vector for all the neurons in computation layer 3 (output layer) \n",
    "W3 = torch.tensor(np.random.uniform(0, 1, (1, 3)), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01217b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.5021, 0.0525, 0.1393, 0.6093, 0.9263, 0.7623, 0.8105, 0.7745, 0.9285,\n",
       "          0.7656],\n",
       "         [0.2872, 0.0415, 0.0759, 0.0043, 0.2576, 0.4515, 0.2372, 0.8354, 0.7372,\n",
       "          0.5003]], dtype=torch.float64, requires_grad=True),\n",
       " tensor([[0.7671, 0.0330],\n",
       "         [0.7451, 0.0602],\n",
       "         [0.1711, 0.8942]], dtype=torch.float64, requires_grad=True),\n",
       " tensor([[0.5432, 0.6121, 0.4518]], dtype=torch.float64, requires_grad=True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define list of weights\n",
    "W_list = [W1, W2, W3]\n",
    "W_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f101adfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.6741], dtype=torch.float64, grad_fn=<MvBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute forward step\n",
    "forwardStep(X[0,:], W_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff07d176",
   "metadata": {},
   "source": [
    "As we have single neuron in output layer. Let's say if we have 5 neuron in output layer then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91f4f083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.0403, 2.6145, 4.7996, 0.8383, 3.5957], dtype=torch.float64,\n",
       "       grad_fn=<MvBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# W3 is all the weigths vector for all the neurons in computation layer 3 (output layer) \n",
    "W3 = torch.tensor(np.random.uniform(0, 1, (5, 3)), requires_grad=True)\n",
    "\n",
    "# define list of weights\n",
    "W_list = [W1, W2, W3]\n",
    "\n",
    "# compute forward step\n",
    "forwardStep(X[0,:], W_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7213ed51",
   "metadata": {},
   "source": [
    "As we can see 5 output for each neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed0a7a4",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b7d3b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2100]) tensor([0.4477])\n"
     ]
    }
   ],
   "source": [
    "activationFunc = nn.Sigmoid()\n",
    "x = torch.randn(1)\n",
    "print(x, activationFunc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6f5b9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([98.7952]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "x = 100 * torch.randn(1)\n",
    "print(x, activationFunc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bef58a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5339]) tensor([0.5339])\n"
     ]
    }
   ],
   "source": [
    "activationFunc = nn.ReLU()\n",
    "x = torch.randn(1)\n",
    "print(x, activationFunc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e1c2d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0939]) tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "x = -1*torch.randn(1)\n",
    "print(x, activationFunc(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e981ebfb",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a12f7151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.79510498046875\n"
     ]
    }
   ],
   "source": [
    "activationFunc = nn.Sigmoid()\n",
    "x = 100*torch.randn(1)\n",
    "y = torch.randint(0, 2, (1,), dtype=torch.float) # Binary Output\n",
    "y_hat = activationFunc(x)\n",
    "lossFunc = nn.BCELoss() # Binary Cross Entropy Loss\n",
    "loss_value = lossFunc(y_hat, y)\n",
    "print(loss_value.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90620d92",
   "metadata": {},
   "source": [
    "### Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62a72230",
   "metadata": {},
   "outputs": [],
   "source": [
    "activationFunc = nn.Sigmoid()\n",
    "lossFunc = nn.BCELoss() # Binary Cross Entropy Loss\n",
    "learningRate = 0.0001\n",
    "x = torch.randn(1)\n",
    "y = torch.randint(0, 2, (1,), dtype=torch.float) # Binary Output\n",
    "w = torch.randn(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c75e6634",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8248857855796814\n",
      "0.8248825073242188\n",
      "0.8248792290687561\n",
      "0.8248760104179382\n",
      "0.8248727321624756\n",
      "0.8248694539070129\n",
      "0.8248661756515503\n",
      "0.8248629570007324\n",
      "0.8248596787452698\n",
      "0.8248564004898071\n",
      "0.8248531222343445\n",
      "0.8248498439788818\n",
      "0.824846625328064\n",
      "0.8248433470726013\n",
      "0.8248400688171387\n",
      "0.824836790561676\n",
      "0.8248335719108582\n",
      "0.8248302936553955\n",
      "0.8248270153999329\n",
      "0.8248237371444702\n",
      "0.8248205184936523\n",
      "0.8248172402381897\n",
      "0.824813961982727\n",
      "0.8248106837272644\n",
      "0.8248074650764465\n",
      "0.8248041868209839\n",
      "0.8248009085655212\n",
      "0.8247976303100586\n",
      "0.8247944116592407\n",
      "0.8247911334037781\n",
      "0.8247878551483154\n",
      "0.8247845768928528\n",
      "0.8247813582420349\n",
      "0.8247780799865723\n",
      "0.8247748017311096\n",
      "0.824771523475647\n",
      "0.8247683048248291\n",
      "0.8247650265693665\n",
      "0.8247617483139038\n",
      "0.8247584700584412\n",
      "0.8247552514076233\n",
      "0.8247518539428711\n",
      "0.8247485756874084\n",
      "0.8247452974319458\n",
      "0.8247420191764832\n",
      "0.8247388005256653\n",
      "0.8247355222702026\n",
      "0.82473224401474\n",
      "0.8247289657592773\n",
      "0.824725866317749\n",
      "0.8247225880622864\n",
      "0.8247193098068237\n",
      "0.8247160911560059\n",
      "0.8247128129005432\n",
      "0.8247095346450806\n",
      "0.8247062563896179\n",
      "0.8247030377388\n",
      "0.8246997594833374\n",
      "0.8246964812278748\n",
      "0.8246932029724121\n",
      "0.8246899843215942\n",
      "0.8246867060661316\n",
      "0.824683427810669\n",
      "0.8246801495552063\n",
      "0.8246769309043884\n",
      "0.8246736526489258\n",
      "0.8246703743934631\n",
      "0.8246669769287109\n",
      "0.8246636986732483\n",
      "0.8246604800224304\n",
      "0.8246572017669678\n",
      "0.8246539235115051\n",
      "0.8246508240699768\n",
      "0.8246475458145142\n",
      "0.8246442675590515\n",
      "0.8246408700942993\n",
      "0.8246375918388367\n",
      "0.8246343731880188\n",
      "0.8246310949325562\n",
      "0.8246278166770935\n",
      "0.8246245384216309\n",
      "0.824621319770813\n",
      "0.8246180415153503\n",
      "0.8246147632598877\n",
      "0.824611485004425\n",
      "0.8246082663536072\n",
      "0.8246049880981445\n",
      "0.8246017098426819\n",
      "0.8245984315872192\n",
      "0.8245952129364014\n",
      "0.8245919346809387\n",
      "0.8245886564254761\n",
      "0.8245853781700134\n",
      "0.8245822787284851\n",
      "0.8245790004730225\n",
      "0.8245757818222046\n",
      "0.8245725035667419\n",
      "0.8245692253112793\n",
      "0.8245659470558167\n",
      "0.8245627284049988\n",
      "0.8245593309402466\n",
      "0.8245560526847839\n",
      "0.8245527744293213\n",
      "0.8245494961738586\n",
      "0.8245462775230408\n",
      "0.8245429992675781\n",
      "0.8245397210121155\n",
      "0.8245364427566528\n",
      "0.824533224105835\n",
      "0.8245299458503723\n",
      "0.8245266675949097\n",
      "0.824523389339447\n",
      "0.8245201706886292\n",
      "0.8245168924331665\n",
      "0.8245136141777039\n",
      "0.8245103359222412\n",
      "0.8245071172714233\n",
      "0.8245038390159607\n",
      "0.824500560760498\n",
      "0.8244973421096802\n",
      "0.8244940638542175\n",
      "0.8244907855987549\n",
      "0.8244875073432922\n",
      "0.8244842886924744\n",
      "0.8244810104370117\n",
      "0.8244777321815491\n",
      "0.8244744539260864\n",
      "0.8244712352752686\n",
      "0.8244679570198059\n",
      "0.8244646787643433\n",
      "0.8244614005088806\n",
      "0.8244581818580627\n",
      "0.8244549036026001\n",
      "0.8244516253471375\n",
      "0.8244482278823853\n",
      "0.8244449496269226\n",
      "0.8244417309761047\n",
      "0.8244384527206421\n",
      "0.8244353532791138\n",
      "0.8244320750236511\n",
      "0.8244287967681885\n",
      "0.8244255185127258\n",
      "0.824422299861908\n",
      "0.8244190216064453\n",
      "0.8244157433509827\n",
      "0.82441246509552\n",
      "0.8244092464447021\n",
      "0.8244059681892395\n",
      "0.8244026899337769\n",
      "0.8243994116783142\n",
      "0.8243961930274963\n",
      "0.8243929147720337\n",
      "0.824389636516571\n",
      "0.8243864178657532\n",
      "0.8243831396102905\n",
      "0.8243798613548279\n",
      "0.8243765830993652\n",
      "0.8243733644485474\n",
      "0.8243700861930847\n",
      "0.8243668079376221\n",
      "0.8243634104728699\n",
      "0.8243601322174072\n",
      "0.8243569135665894\n",
      "0.8243536353111267\n",
      "0.8243503570556641\n",
      "0.8243470788002014\n",
      "0.8243438601493835\n",
      "0.8243405818939209\n",
      "0.8243373036384583\n",
      "0.8243340849876404\n",
      "0.8243309259414673\n",
      "0.8243276476860046\n",
      "0.8243244290351868\n",
      "0.8243211507797241\n",
      "0.8243178725242615\n",
      "0.8243145942687988\n",
      "0.824311375617981\n",
      "0.8243080973625183\n",
      "0.8243048191070557\n",
      "0.8243014216423035\n",
      "0.8242982029914856\n",
      "0.824294924736023\n",
      "0.8242916464805603\n",
      "0.8242883682250977\n",
      "0.8242851495742798\n",
      "0.8242818713188171\n",
      "0.8242785930633545\n",
      "0.8242753148078918\n",
      "0.824272096157074\n",
      "0.8242688179016113\n",
      "0.8242655396461487\n",
      "0.8242623209953308\n",
      "0.8242590427398682\n",
      "0.8242557644844055\n",
      "0.8242524862289429\n",
      "0.824249267578125\n",
      "0.8242459893226624\n",
      "0.8242427110671997\n",
      "0.8242394924163818\n",
      "0.8242362141609192\n",
      "0.8242330551147461\n",
      "0.8242296576499939\n",
      "0.824226438999176\n",
      "0.8242231607437134\n",
      "0.8242198824882507\n",
      "0.8242166042327881\n",
      "0.8242133855819702\n",
      "0.8242101073265076\n",
      "0.8242068290710449\n",
      "0.824203610420227\n",
      "0.8242003321647644\n",
      "0.8241970539093018\n",
      "0.8241937756538391\n",
      "0.8241905570030212\n",
      "0.8241872787475586\n",
      "0.824184000492096\n",
      "0.8241807818412781\n",
      "0.8241775035858154\n",
      "0.8241742253303528\n",
      "0.8241709470748901\n",
      "0.8241677284240723\n",
      "0.8241644501686096\n",
      "0.824161171913147\n",
      "0.8241578936576843\n",
      "0.8241546750068665\n",
      "0.8241513967514038\n",
      "0.8241481184959412\n",
      "0.8241448998451233\n",
      "0.8241415023803711\n",
      "0.8241382241249084\n",
      "0.8241349458694458\n",
      "0.8241316676139832\n",
      "0.8241285681724548\n",
      "0.8241252899169922\n",
      "0.8241220712661743\n",
      "0.8241187930107117\n",
      "0.824115514755249\n",
      "0.8241122364997864\n",
      "0.8241090178489685\n",
      "0.8241057395935059\n",
      "0.8241024613380432\n",
      "0.8240992426872253\n",
      "0.8240959644317627\n",
      "0.8240926861763\n",
      "0.8240892887115479\n",
      "0.8240861892700195\n",
      "0.8240829110145569\n",
      "0.8240796327590942\n",
      "0.8240764141082764\n",
      "0.8240730166435242\n",
      "0.8240697383880615\n",
      "0.8240664601325989\n",
      "0.8240631818771362\n",
      "0.8240599632263184\n",
      "0.8240566849708557\n",
      "0.8240534067153931\n",
      "0.8240501880645752\n",
      "0.8240470290184021\n",
      "0.8240437507629395\n",
      "0.8240405321121216\n",
      "0.8240372538566589\n",
      "0.8240339756011963\n",
      "0.8240307569503784\n",
      "0.8240274786949158\n",
      "0.8240240812301636\n",
      "0.8240208029747009\n",
      "0.8240175247192383\n",
      "0.8240143060684204\n",
      "0.8240110278129578\n",
      "0.8240077495574951\n",
      "0.8240045309066772\n",
      "0.8240012526512146\n",
      "0.823997974395752\n",
      "0.8239946961402893\n",
      "0.8239914774894714\n",
      "0.8239881992340088\n",
      "0.8239849209785461\n",
      "0.8239817023277283\n",
      "0.8239784240722656\n",
      "0.823975145816803\n",
      "0.8239718675613403\n",
      "0.823968768119812\n",
      "0.8239654898643494\n",
      "0.8239620923995972\n",
      "0.8239588737487793\n",
      "0.8239555954933167\n",
      "0.823952317237854\n",
      "0.8239490985870361\n",
      "0.8239458203315735\n",
      "0.8239425420761108\n",
      "0.8239392638206482\n",
      "0.8239360451698303\n",
      "0.8239327669143677\n",
      "0.823929488658905\n",
      "0.8239262700080872\n",
      "0.8239229917526245\n",
      "0.8239197134971619\n",
      "0.8239164352416992\n",
      "0.8239132165908813\n",
      "0.8239099383354187\n",
      "0.823906660079956\n",
      "0.8239034414291382\n",
      "0.8239001631736755\n",
      "0.8238968849182129\n",
      "0.823893666267395\n",
      "0.8238903880119324\n",
      "0.8238869905471802\n",
      "0.8238837122917175\n",
      "0.8238806128501892\n",
      "0.8238773345947266\n",
      "0.8238740563392639\n",
      "0.823870837688446\n",
      "0.8238675594329834\n",
      "0.8238642811775208\n",
      "0.8238610029220581\n",
      "0.8238577842712402\n",
      "0.8238545060157776\n",
      "0.8238512277603149\n",
      "0.8238480091094971\n",
      "0.8238447308540344\n",
      "0.8238414525985718\n",
      "0.8238382339477539\n",
      "0.8238349556922913\n",
      "0.8238316774368286\n",
      "0.8238282799720764\n",
      "0.8238250017166138\n",
      "0.8238217830657959\n",
      "0.8238185048103333\n",
      "0.8238152265548706\n",
      "0.8238120079040527\n",
      "0.8238088488578796\n",
      "0.8238056302070618\n",
      "0.8238023519515991\n",
      "0.8237990736961365\n",
      "0.8237957954406738\n",
      "0.823792576789856\n",
      "0.8237891793251038\n",
      "0.8237859010696411\n",
      "0.8237826228141785\n",
      "0.8237794041633606\n",
      "0.823776125907898\n",
      "0.8237728476524353\n",
      "0.8237696290016174\n",
      "0.8237663507461548\n",
      "0.8237630724906921\n",
      "0.8237598538398743\n",
      "0.8237565755844116\n",
      "0.823753297328949\n",
      "0.8237500190734863\n",
      "0.8237468004226685\n",
      "0.8237436413764954\n",
      "0.8237404227256775\n",
      "0.8237371444702148\n",
      "0.8237337470054626\n",
      "0.82373046875\n",
      "0.8237272500991821\n",
      "0.8237239718437195\n",
      "0.8237206935882568\n",
      "0.8237174153327942\n",
      "0.8237141966819763\n",
      "0.8237109184265137\n",
      "0.823707640171051\n",
      "0.8237044215202332\n",
      "0.8237011432647705\n",
      "0.8236978650093079\n",
      "0.82369464635849\n",
      "0.8236913681030273\n",
      "0.8236880898475647\n",
      "0.8236848711967468\n",
      "0.8236815929412842\n",
      "0.8236783146858215\n",
      "0.8236750364303589\n",
      "0.823671817779541\n",
      "0.8236684203147888\n",
      "0.8236652612686157\n",
      "0.8236620426177979\n",
      "0.8236587643623352\n",
      "0.8236554861068726\n",
      "0.8236522674560547\n",
      "0.823648989200592\n",
      "0.8236457109451294\n",
      "0.8236424922943115\n",
      "0.8236392140388489\n",
      "0.8236359357833862\n",
      "0.8236326575279236\n",
      "0.8236294388771057\n",
      "0.8236261606216431\n",
      "0.8236228823661804\n",
      "0.8236194849014282\n",
      "0.8236162662506104\n",
      "0.8236129879951477\n",
      "0.8236097097396851\n",
      "0.8236066102981567\n",
      "0.8236033320426941\n",
      "0.8236001133918762\n",
      "0.8235968351364136\n",
      "0.8235935568809509\n",
      "0.8235903382301331\n",
      "0.8235870599746704\n",
      "0.8235836625099182\n",
      "0.8235803842544556\n",
      "0.8235771059989929\n",
      "0.8235740065574646\n",
      "0.8235706090927124\n",
      "0.8235673308372498\n",
      "0.8235641121864319\n",
      "0.8235608339309692\n",
      "0.8235575556755066\n",
      "0.8235543370246887\n",
      "0.8235510587692261\n",
      "0.8235477805137634\n",
      "0.8235446810722351\n",
      "0.8235414028167725\n",
      "0.8235381841659546\n",
      "0.8235347867012024\n",
      "0.8235315084457397\n",
      "0.8235282301902771\n",
      "0.8235249519348145\n",
      "0.8235217332839966\n",
      "0.8235184550285339\n",
      "0.8235151767730713\n",
      "0.8235119581222534\n",
      "0.8235086798667908\n",
      "0.8235054016113281\n",
      "0.8235021829605103\n",
      "0.8234989047050476\n",
      "0.823495626449585\n",
      "0.8234924077987671\n",
      "0.823489248752594\n",
      "0.8234858512878418\n",
      "0.8234826326370239\n",
      "0.8234793543815613\n",
      "0.8234760761260986\n",
      "0.8234728574752808\n",
      "0.8234695792198181\n",
      "0.8234663009643555\n",
      "0.8234630823135376\n",
      "0.823459804058075\n",
      "0.8234565258026123\n",
      "0.8234532475471497\n",
      "0.8234500288963318\n",
      "0.8234467506408691\n",
      "0.8234434723854065\n",
      "0.8234402537345886\n",
      "0.823436975479126\n",
      "0.8234336972236633\n",
      "0.8234304785728455\n",
      "0.8234270811080933\n",
      "0.8234238028526306\n",
      "0.8234207034111023\n",
      "0.8234174251556396\n",
      "0.823414146900177\n",
      "0.8234109282493591\n",
      "0.8234076499938965\n",
      "0.8234043717384338\n",
      "0.823401153087616\n",
      "0.8233978748321533\n",
      "0.8233945965766907\n",
      "0.8233913779258728\n",
      "0.8233880996704102\n",
      "0.823384702205658\n",
      "0.8233814239501953\n",
      "0.8233782052993774\n",
      "0.8233749270439148\n",
      "0.8233716487884521\n",
      "0.8233684301376343\n",
      "0.8233652710914612\n",
      "0.8233619928359985\n",
      "0.8233587741851807\n",
      "0.823355495929718\n",
      "0.8233520984649658\n",
      "0.823348879814148\n",
      "0.8233456015586853\n",
      "0.8233423233032227\n",
      "0.82333904504776\n",
      "0.8233358263969421\n",
      "0.8233325481414795\n",
      "0.8233292698860168\n",
      "0.823326051235199\n",
      "0.8233227729797363\n",
      "0.8233194947242737\n",
      "0.8233162760734558\n",
      "0.8233131170272827\n",
      "0.8233098983764648\n",
      "0.8233065009117126\n",
      "0.82330322265625\n",
      "0.8232999444007874\n",
      "0.8232967257499695\n",
      "0.8232934474945068\n",
      "0.8232901692390442\n",
      "0.8232869505882263\n",
      "0.8232836723327637\n",
      "0.823280394077301\n",
      "0.8232771754264832\n",
      "0.8232738971710205\n",
      "0.8232706189155579\n",
      "0.82326740026474\n",
      "0.8232641220092773\n",
      "0.8232608437538147\n",
      "0.8232576251029968\n",
      "0.8232542276382446\n",
      "0.8232510685920715\n",
      "0.8232478499412537\n",
      "0.823244571685791\n",
      "0.8232412934303284\n",
      "0.8232380747795105\n",
      "0.8232347965240479\n",
      "0.8232315182685852\n",
      "0.8232282996177673\n",
      "0.8232250213623047\n",
      "0.823221743106842\n",
      "0.8232185244560242\n",
      "0.8232152462005615\n",
      "0.8232118487358093\n",
      "0.8232085704803467\n",
      "0.8232053518295288\n",
      "0.8232021927833557\n",
      "0.8231989741325378\n",
      "0.8231956958770752\n",
      "0.8231924176216125\n",
      "0.8231891989707947\n",
      "0.823185920715332\n",
      "0.8231825232505798\n",
      "0.8231794238090515\n",
      "0.8231761455535889\n",
      "0.8231727480888367\n",
      "0.823169469833374\n",
      "0.8231662511825562\n",
      "0.8231629729270935\n",
      "0.8231596946716309\n",
      "0.823156476020813\n",
      "0.8231533169746399\n",
      "0.823150098323822\n",
      "0.8231468200683594\n",
      "0.8231435418128967\n",
      "0.8231401443481445\n",
      "0.8231369256973267\n",
      "0.823133647441864\n",
      "0.8231303691864014\n",
      "0.8231271505355835\n",
      "0.8231238722801208\n",
      "0.8231205940246582\n",
      "0.8231173753738403\n",
      "0.8231140971183777\n",
      "0.823110818862915\n",
      "0.8231076002120972\n",
      "0.8231044411659241\n",
      "0.8231010437011719\n",
      "0.823097825050354\n",
      "0.8230945467948914\n",
      "0.8230912685394287\n",
      "0.8230880498886108\n",
      "0.8230847716331482\n",
      "0.8230815529823303\n",
      "0.8230782747268677\n",
      "0.823074996471405\n",
      "0.8230717778205872\n",
      "0.8230684995651245\n",
      "0.8230652213096619\n",
      "0.823062002658844\n",
      "0.8230587244033813\n",
      "0.8230554461479187\n",
      "0.8230520486831665\n",
      "0.8230489492416382\n",
      "0.8230456709861755\n",
      "0.8230424523353577\n",
      "0.823039174079895\n",
      "0.8230358958244324\n",
      "0.8230326771736145\n",
      "0.8230293989181519\n",
      "0.8230261206626892\n",
      "0.8230229020118713\n",
      "0.8230196237564087\n",
      "0.823016345500946\n",
      "0.8230129480361938\n",
      "0.823009729385376\n",
      "0.8230064511299133\n",
      "0.823003351688385\n",
      "0.8230000734329224\n",
      "0.8229967951774597\n",
      "0.8229935765266418\n",
      "0.8229902982711792\n",
      "0.822986900806427\n",
      "0.8229836821556091\n",
      "0.822980523109436\n",
      "0.8229771256446838\n",
      "0.822973906993866\n",
      "0.8229706287384033\n",
      "0.8229673504829407\n",
      "0.8229641318321228\n",
      "0.8229608535766602\n",
      "0.8229577541351318\n",
      "0.8229544758796692\n",
      "0.822951078414917\n",
      "0.8229478001594543\n",
      "0.8229445815086365\n",
      "0.8229413032531738\n",
      "0.8229380249977112\n",
      "0.8229348063468933\n",
      "0.8229315280914307\n",
      "0.822928249835968\n",
      "0.8229250311851501\n",
      "0.8229217529296875\n",
      "0.8229184746742249\n",
      "0.8229153752326965\n",
      "0.8229119777679443\n",
      "0.8229086995124817\n",
      "0.8229054808616638\n",
      "0.8229022026062012\n",
      "0.8228989839553833\n",
      "0.8228957056999207\n",
      "0.822892427444458\n",
      "0.8228892087936401\n",
      "0.8228859305381775\n",
      "0.8228826522827148\n",
      "0.822879433631897\n",
      "0.8228761553764343\n",
      "0.8228728771209717\n",
      "0.8228696584701538\n",
      "0.8228662610054016\n",
      "0.8228631019592285\n",
      "0.8228598833084106\n",
      "0.822856605052948\n",
      "0.8228533267974854\n",
      "0.8228501081466675\n",
      "0.8228468298912048\n",
      "0.822843611240387\n",
      "0.8228403329849243\n",
      "0.8228370547294617\n",
      "0.8228338360786438\n",
      "0.8228304386138916\n",
      "0.822827160358429\n",
      "0.8228238821029663\n",
      "0.822820782661438\n",
      "0.8228175044059753\n",
      "0.8228142857551575\n",
      "0.8228110074996948\n",
      "0.8228077292442322\n",
      "0.82280433177948\n",
      "0.8228011131286621\n",
      "0.8227978348731995\n",
      "0.8227946162223816\n",
      "0.822791337966919\n",
      "0.8227880597114563\n",
      "0.8227848410606384\n",
      "0.8227815628051758\n",
      "0.8227784633636475\n",
      "0.8227751851081848\n",
      "0.8227717876434326\n",
      "0.82276850938797\n",
      "0.8227652907371521\n",
      "0.8227620124816895\n",
      "0.8227587342262268\n",
      "0.8227555155754089\n",
      "0.8227522373199463\n",
      "0.8227490186691284\n",
      "0.8227457404136658\n",
      "0.8227424621582031\n",
      "0.8227392435073853\n",
      "0.8227359652519226\n",
      "0.82273268699646\n",
      "0.8227294683456421\n",
      "0.8227261900901794\n",
      "0.8227229118347168\n",
      "0.8227196931838989\n",
      "0.8227164149284363\n",
      "0.8227131366729736\n",
      "0.8227099180221558\n",
      "0.8227066397666931\n",
      "0.8227034211158752\n",
      "0.8227001428604126\n",
      "0.82269686460495\n",
      "0.8226934671401978\n",
      "0.8226903676986694\n",
      "0.8226870894432068\n",
      "0.8226838707923889\n",
      "0.8226805925369263\n",
      "0.8226773142814636\n",
      "0.8226740956306458\n",
      "0.8226708173751831\n",
      "0.8226675987243652\n",
      "0.8226643204689026\n",
      "0.8226609230041504\n",
      "0.8226576447486877\n",
      "0.8226544260978699\n",
      "0.8226511478424072\n",
      "0.8226480484008789\n",
      "0.8226447701454163\n",
      "0.8226414918899536\n",
      "0.8226382732391357\n",
      "0.8226348757743835\n",
      "0.8226315975189209\n",
      "0.822628378868103\n",
      "0.8226251006126404\n",
      "0.8226218223571777\n",
      "0.8226186037063599\n",
      "0.8226153254508972\n",
      "0.8226120471954346\n",
      "0.8226089477539062\n",
      "0.8226056694984436\n",
      "0.8226023316383362\n",
      "0.8225990533828735\n",
      "0.8225957751274109\n",
      "0.822592556476593\n",
      "0.8225892782211304\n",
      "0.8225859999656677\n",
      "0.8225827813148499\n",
      "0.8225795030593872\n",
      "0.8225762248039246\n",
      "0.8225730061531067\n",
      "0.822569727897644\n",
      "0.8225665092468262\n",
      "0.8225632309913635\n",
      "0.8225599527359009\n",
      "0.822556734085083\n",
      "0.8225534558296204\n",
      "0.8225501775741577\n",
      "0.8225469589233398\n",
      "0.8225436806678772\n",
      "0.8225404620170593\n",
      "0.8225371837615967\n",
      "0.822533905506134\n",
      "0.8225305080413818\n",
      "0.822527289390564\n",
      "0.8225241303443909\n",
      "0.822520911693573\n",
      "0.8225176334381104\n",
      "0.8225144147872925\n",
      "0.8225111365318298\n",
      "0.8225078582763672\n",
      "0.8225046396255493\n",
      "0.8225013613700867\n",
      "0.8224979639053345\n",
      "0.8224946856498718\n",
      "0.822491466999054\n",
      "0.8224883079528809\n",
      "0.822485089302063\n",
      "0.8224818110466003\n",
      "0.8224785923957825\n",
      "0.8224751949310303\n",
      "0.8224719166755676\n",
      "0.8224688172340393\n",
      "0.8224654197692871\n",
      "0.8224621415138245\n",
      "0.8224589228630066\n",
      "0.822455644607544\n",
      "0.8224523663520813\n",
      "0.822449266910553\n",
      "0.8224459886550903\n",
      "0.8224425911903381\n",
      "0.8224393725395203\n",
      "0.8224360942840576\n",
      "0.8224328756332397\n",
      "0.8224295973777771\n",
      "0.8224263191223145\n",
      "0.8224231004714966\n",
      "0.8224198222160339\n",
      "0.8224165439605713\n",
      "0.8224133253097534\n",
      "0.8224100470542908\n",
      "0.8224068284034729\n",
      "0.8224035501480103\n",
      "0.8224002718925476\n",
      "0.8223970532417297\n",
      "0.8223937749862671\n",
      "0.8223904967308044\n",
      "0.8223872780799866\n",
      "0.8223839998245239\n",
      "0.822380781173706\n",
      "0.8223775029182434\n",
      "0.8223741054534912\n",
      "0.8223710060119629\n",
      "0.8223677277565002\n",
      "0.8223644495010376\n",
      "0.8223612308502197\n",
      "0.8223579525947571\n",
      "0.8223547339439392\n",
      "0.8223514556884766\n",
      "0.8223481774330139\n",
      "0.8223447799682617\n",
      "0.8223415613174438\n",
      "0.8223382830619812\n",
      "0.8223351836204529\n",
      "0.8223319053649902\n",
      "0.8223286867141724\n",
      "0.8223254084587097\n",
      "0.8223220109939575\n",
      "0.8223189115524292\n",
      "0.8223156332969666\n",
      "0.8223122358322144\n",
      "0.8223090171813965\n",
      "0.8223057389259338\n",
      "0.8223024606704712\n",
      "0.8222993612289429\n",
      "0.8222960829734802\n",
      "0.8222927451133728\n",
      "0.8222894668579102\n",
      "0.8222861886024475\n",
      "0.8222829699516296\n",
      "0.822279691696167\n",
      "0.8222764134407043\n",
      "0.8222731947898865\n",
      "0.8222699165344238\n",
      "0.822266697883606\n",
      "0.8222634196281433\n",
      "0.8222601413726807\n",
      "0.8222569227218628\n",
      "0.8222536444664001\n",
      "0.8222504258155823\n",
      "0.8222471475601196\n",
      "0.822243869304657\n",
      "0.8222406506538391\n",
      "0.8222373723983765\n",
      "0.8222341537475586\n",
      "0.822230875492096\n",
      "0.8222274780273438\n",
      "0.8222243785858154\n",
      "0.8222211003303528\n",
      "0.8222178220748901\n",
      "0.8222146034240723\n",
      "0.8222113251686096\n",
      "0.8222081065177917\n",
      "0.8222048282623291\n",
      "0.8222015500068665\n",
      "0.8221981525421143\n",
      "0.8221949338912964\n",
      "0.8221916556358337\n",
      "0.8221885561943054\n",
      "0.8221852779388428\n",
      "0.8221820592880249\n",
      "0.8221787810325623\n",
      "0.8221753835678101\n",
      "0.8221722841262817\n",
      "0.8221688866615295\n",
      "0.8221656084060669\n",
      "0.822162389755249\n",
      "0.8221591114997864\n",
      "0.8221560120582581\n",
      "0.8221527338027954\n",
      "0.8221493363380432\n",
      "0.8221461176872253\n",
      "0.8221428394317627\n",
      "0.8221396207809448\n",
      "0.8221363425254822\n",
      "0.8221330642700195\n",
      "0.8221298456192017\n",
      "0.822126567363739\n",
      "0.8221234679222107\n",
      "0.8221200704574585\n",
      "0.8221167922019958\n",
      "0.822113573551178\n",
      "0.8221102952957153\n",
      "0.8221070170402527\n",
      "0.8221037983894348\n",
      "0.8221005201339722\n",
      "0.8220973014831543\n",
      "0.8220940232276917\n",
      "0.822090744972229\n",
      "0.8220875263214111\n",
      "0.8220842480659485\n",
      "0.8220810294151306\n",
      "0.822077751159668\n",
      "0.8220744729042053\n",
      "0.8220712542533875\n",
      "0.8220679759979248\n",
      "0.8220647573471069\n",
      "0.8220614790916443\n",
      "0.8220580816268921\n",
      "0.8220548033714294\n",
      "0.8220517039299011\n",
      "0.8220484852790833\n",
      "0.8220452070236206\n",
      "0.822041928768158\n",
      "0.8220385313034058\n",
      "0.8220354318618774\n",
      "0.8220322132110596\n",
      "0.8220288157463074\n",
      "0.8220255374908447\n",
      "0.8220222592353821\n",
      "0.8220191597938538\n",
      "0.8220158815383911\n",
      "0.8220126628875732\n",
      "0.822009265422821\n",
      "0.8220059871673584\n",
      "0.8220027685165405\n",
      "0.8219994902610779\n",
      "0.82199627161026\n",
      "0.8219929933547974\n",
      "0.8219897150993347\n",
      "0.8219866156578064\n",
      "0.8219832181930542\n",
      "0.8219799995422363\n",
      "0.8219767212867737\n",
      "0.821973443031311\n",
      "0.8219702243804932\n",
      "0.8219669461250305\n",
      "0.8219637274742126\n",
      "0.82196044921875\n",
      "0.8219571709632874\n",
      "0.8219539523124695\n",
      "0.8219506740570068\n",
      "0.821947455406189\n",
      "0.8219441771507263\n",
      "0.8219408988952637\n",
      "0.8219376802444458\n",
      "0.8219344019889832\n",
      "0.8219311833381653\n",
      "0.8219279050827026\n",
      "0.82192462682724\n",
      "0.8219212889671326\n",
      "0.8219181299209595\n",
      "0.8219149112701416\n",
      "0.821911633014679\n",
      "0.8219083547592163\n",
      "0.8219051361083984\n",
      "0.8219018578529358\n",
      "0.8218986392021179\n",
      "0.8218952417373657\n",
      "0.8218919634819031\n",
      "0.8218887448310852\n",
      "0.8218855857849121\n",
      "0.8218823671340942\n",
      "0.8218790888786316\n",
      "0.8218756914138794\n",
      "0.8218724727630615\n",
      "0.8218691945075989\n",
      "0.8218659162521362\n",
      "0.8218626976013184\n",
      "0.8218594193458557\n",
      "0.8218563199043274\n",
      "0.8218530416488647\n",
      "0.8218496441841125\n",
      "0.8218464255332947\n",
      "0.821843147277832\n",
      "0.8218399286270142\n",
      "0.8218366503715515\n",
      "0.8218334317207336\n",
      "0.821830153465271\n",
      "0.8218268752098083\n",
      "0.8218236565589905\n",
      "0.8218203783035278\n",
      "0.82181715965271\n",
      "0.8218138813972473\n",
      "0.8218106031417847\n",
      "0.8218073844909668\n",
      "0.8218041062355042\n",
      "0.8218008875846863\n",
      "0.8217976093292236\n",
      "0.821794331073761\n",
      "0.8217909932136536\n",
      "0.8217878341674805\n",
      "0.8217846155166626\n",
      "0.8217813372612\n",
      "0.8217780590057373\n",
      "0.8217748403549194\n",
      "0.8217715620994568\n",
      "0.8217683434486389\n",
      "0.8217649459838867\n",
      "0.8217616677284241\n",
      "0.8217585682868958\n",
      "0.8217552900314331\n",
      "0.8217520713806152\n",
      "0.821748673915863\n",
      "0.8217453956604004\n",
      "0.8217421770095825\n",
      "0.8217388987541199\n",
      "0.821735680103302\n",
      "0.8217324018478394\n",
      "0.821729302406311\n",
      "0.8217260241508484\n",
      "0.8217226266860962\n",
      "0.8217194080352783\n",
      "0.8217161297798157\n",
      "0.821712851524353\n",
      "0.8217096328735352\n",
      "0.8217063546180725\n",
      "0.8217031359672546\n",
      "0.8216999769210815\n",
      "0.8216966390609741\n",
      "0.8216933608055115\n",
      "0.8216900825500488\n",
      "0.821686863899231\n",
      "0.8216835856437683\n",
      "0.8216803669929504\n",
      "0.8216770887374878\n",
      "0.8216738104820251\n",
      "0.8216705918312073\n",
      "0.8216671943664551\n",
      "0.8216640949249268\n",
      "0.8216608166694641\n",
      "0.8216575980186462\n",
      "0.8216543197631836\n",
      "0.821651041507721\n",
      "0.8216478228569031\n",
      "0.8216445446014404\n",
      "0.8216411471366882\n",
      "0.8216379284858704\n",
      "0.8216347694396973\n",
      "0.8216315507888794\n",
      "0.8216282725334167\n"
     ]
    }
   ],
   "source": [
    "nIter = 1000\n",
    "for index in range(nIter):\n",
    "    y_hat = activationFunc(w*x)\n",
    "    loss = lossFunc(y_hat, y)\n",
    "    loss.backward()\n",
    "    dw = w.grad.data\n",
    "    with torch.no_grad():\n",
    "        w -= learningRate*dw\n",
    "    w.grad.data.zero_()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2db44028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(x):\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42499485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParams(W_list, dW_list, lr):\n",
    "    with torch.no_grad():\n",
    "        for index in range(len(W_list)):\n",
    "            W_list[index] -= lr*dW_list[index]\n",
    "    return W_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ffc58462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute weighted sum of neurons of a layer\n",
    "def forwardStepNN(X, W_list):\n",
    "    h = torch.from_numpy(X)\n",
    "    for W in W_list:\n",
    "        z = torch.matmul(W,h)\n",
    "        h = activate(z)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4402f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN_sgd(X, y, W_list, loss_fn, lr=0.0001, epochs=100):\n",
    "    for epoch in range(epochs):\n",
    "        avgLoss = []\n",
    "        for i in range(len(y)):\n",
    "            X_in = X[i,:]\n",
    "            yTrue = y[i]\n",
    "            print(yTrue)\n",
    "            y_hat = forwardStepNN(X_in, W_list)\n",
    "            print(y_hat)\n",
    "            loss = loss_fn(y_hat, torch.tensor(yTrue, dtype=torch.double))\n",
    "            loss.backword()\n",
    "            avgLoss.append(loss.item())\n",
    "            sys.stdout.flush()\n",
    "            for j in range(len(W_list)):\n",
    "                dW_list.append(W_list[j].grad.data)\n",
    "            W_list = updateParams(W_list, dW_list, lr)\n",
    "            for k in range(len(W_list)):\n",
    "                W_list[k].grad.data.zero_()\n",
    "        print('Loss after epoch=%d: %f' %(epoch, np.mean(np.array(avgLoss))))\n",
    "    return W_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "811d80b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([0.5584], dtype=torch.float64, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m W_list \u001b[38;5;241m=\u001b[39m [W1, W2, W3]\n\u001b[0;32m     19\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss() \u001b[38;5;66;03m# Binary Cross Entropy Loss\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mtrainNN_sgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36mtrainNN_sgd\u001b[1;34m(X, y, W_list, loss_fn, lr, epochs)\u001b[0m\n\u001b[0;32m      8\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m forwardStepNN(X_in, W_list)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_hat)\n\u001b[1;32m---> 10\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43myTrue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackword()\n\u001b[0;32m     12\u001b[0m avgLoss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\torch\\nn\\modules\\loss.py:612\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 612\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\torch\\nn\\functional.py:3056\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3054\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m-> 3056\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3057\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3059\u001b[0m     )\n\u001b[0;32m   3061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3062\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "# Stocastic Gradient Decent\n",
    "inputDim = 10\n",
    "n = 1000\n",
    "X = np.random.rand(n, inputDim)\n",
    "y = np.random.randint(0, 2, n)\n",
    "\n",
    "# W1 is all the weigths vector for all the neurons in computation layer 1 \n",
    "W1 = torch.tensor(np.random.uniform(0, 1, (2, inputDim)), requires_grad=True)\n",
    "\n",
    "# W2 is all the weigths vector for all the neurons in computation layer 2 \n",
    "W2 = torch.tensor(np.random.uniform(0, 1, (3, 2)), requires_grad=True)\n",
    "\n",
    "# W3 is all the weigths vector for all the neurons in computation layer 3 (output layer) \n",
    "W3 = torch.tensor(np.random.uniform(0, 1, (1, 3)), requires_grad=True)\n",
    "\n",
    "# define list of weights\n",
    "W_list = [W1, W2, W3]\n",
    "\n",
    "loss_fn = nn.BCELoss() # Binary Cross Entropy Loss\n",
    "trainNN_sgd(X, y, W_list, loss_fn, lr=0.0001, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b572fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
